# -*- coding: utf-8 -*-
"""FakeNews_UTK_DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NLhceC1RQiKg0KGOdY8Y-R6XDEXc99AK

**Author Shafqaat Ahmad@PennState**
Fake news detection using UTK dataset

##BERT-CNN

Here we implement BERT-CNN language model by using the BERT embeddings as a layer in CNN
"""

# Install the necessary libraries 

!pip install bert-for-tf2
!pip install sentencepiece

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Import the necessary libraries 

try:
#     %tensorflow_version 2.x
except Exception:
    pass
import tensorflow as tf

import tensorflow_hub as hub

from tensorflow.keras import layers
import bert
import pandas as pd
import numpy as np
import re
import random
import math
import seaborn as sns
import matplotlib.pyplot as plt

import random as rn

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(2902)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(4900545)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,
                                        inter_op_parallelism_threads=1)

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see:
# https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.compat.v1.set_random_seed(4900545)

sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

# Load the fake news dataset

fake_news = pd.read_csv("/content/drive/MyDrive/FakeNewsDataSets/data.csv")

fake_news.isnull().values.any() # check for null or none values

fake_news.shape

fake_news = fake_news.iloc[:,[3,-1]] # select the labels and texts from the dataset

fake_news.head()

fake_news.columns = ["text", "label"]

fake_news = fake_news.sample(frac=0.2) # select 20% of the dataset
fake_news.describe()
fake_news.isnull().sum()

fake_news.dropna(inplace=True) # remove NaN values
fake_news.isnull().sum()


# plot the data distribution
sns.countplot(x="label", data=fake_news);
plt.show()

# connect to google drive from colab to be able to load the data

from google.colab import drive
drive.mount('/content/drive')

# function for performing preprocess of texts

def preprocess_text(sen):
    # Removing html tags
    sentence = remove_tags(sen)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

# apply regular expression to remove uncessary characters like ><
TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', str(text)) 

news = []
sentences = list(fake_news['text'])

# run the preprocessing function on the data
for sen in sentences:
    news.append(preprocess_text(sen)) 

print(fake_news.columns.values)


fake_news.label.unique()

y = fake_news['label']
y = np.array(list(map(lambda x: 1 if x==1 else 0, y)))
# y = np.array(list(map(lambda x: 1 if x=="positive" else 0, y)))

print(news[10])  

# Load BERT model and the necessary assests
BertTokenizer = bert.bert_tokenization.FullTokenizer # get the tokenizer function from BERT
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2",
                            trainable=False)
vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = BertTokenizer(vocabulary_file, to_lower_case)  # initialize the tokenizer

# apply tokenizer to the fake news text
def tokenize_reviews(text_news):
    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_news)) 

tokenized_news = [tokenize_reviews(row) for row in news]
print(tokenized_news[1:2])
# print(news[1:2])

# prepare the tokenized texts for training
news_with_len = [[news_j, y[i], len(news_j)] for i, news_j in enumerate(tokenized_news)] # arrange the data as: | news_token | text_label | size of token | 

random.shuffle(news_with_len) # apply randomization

news_with_len.sort(key=lambda x: x[2]) # now sort based on ids

sorted_news_labels = [(news_lab[0], news_lab[1]) for news_lab in news_with_len] # select news_token and text_labels

processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_news_labels, output_types=(tf.int32, tf.int32)) # prepare the data into tensor form and augment the data
# print(processed_dataset.batch(64).as_numpy_iterator())

BATCH_SIZE = 64
batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ())) # break data into batches to make train easy

next(iter(batched_dataset))

TOTAL_BATCHES = math.ceil(len(sorted_news_labels) / BATCH_SIZE) # total number of batches
print(TOTAL_BATCHES)
TEST_BATCHES = TOTAL_BATCHES // 6
print(TEST_BATCHES)
batched_dataset.shuffle(TOTAL_BATCHES)
test_data = batched_dataset.take(TEST_BATCHES) # test batch
train_data = batched_dataset.skip(TEST_BATCHES) # train batch

"""CNN"""

# Fucntion to create CNN model
class TEXT_MODEL_CNN(tf.keras.Model):
    
    # CNN base parameters like filter_numbers, dropout, embedding_size etc
    def __init__(self,
                 vocabulary_size,
                 embedding_dimensions=32,
                 cnn_filters=20,
                 dnn_units=64,
                 model_output_classes=2,
                 dropout_rate=0.2,
                 training=False,
                 name="text_model"):
        super(TEXT_MODEL_CNN, self).__init__(name=name)
        
        # embedding layer, this will be embedding from BERT
        self.embedding = layers.Embedding(vocabulary_size,
                                          embedding_dimensions)
        # CNN layers
        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=2,
                                        padding="valid",
                                        activation="relu")
        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=3,
                                        padding="valid",
                                        activation="relu")
        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=4,
                                        padding="valid",
                                        activation="relu")
        self.pool = layers.GlobalMaxPool1D()
        
        self.dense_1 = layers.Dense(units=dnn_units, activation="relu")
        self.dropout = layers.Dropout(rate=dropout_rate)

        # final dense layer (fully connected network)
        if model_output_classes == 2:
            self.last_dense = layers.Dense(units=1,
                                           activation="sigmoid")
        else:
            self.last_dense = layers.Dense(units=model_output_classes,
                                           activation="softmax")
    
    def call(self, inputs, training):
        l = self.embedding(inputs)
        l_1 = self.cnn_layer1(l) 
        l_1 = self.pool(l_1) 
        l_2 = self.cnn_layer2(l) 
        l_2 = self.pool(l_2)
        l_3 = self.cnn_layer3(l)
        l_3 = self.pool(l_3) 
        
        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters) # concatenate the CNN layers
        concatenated = self.dense_1(concatenated)
        concatenated = self.dropout(concatenated, training)
        model_output = self.last_dense(concatenated)
        
        return model_output

# Training with CNN starts here

# initial parameters
VOCAB_LENGTH = len(tokenizer.vocab) # vocab len is equivalent to the one in BERT
EMB_DIM = 500
CNN_FILTERS = 200
DNN_UNITS = 256
OUTPUT_CLASSES = 2

DROPOUT_RATE = 0.2

NB_EPOCHS = 10

# initialize the model
text_model = TEXT_MODEL_CNN(vocabulary_size=VOCAB_LENGTH,
                        embedding_dimensions=EMB_DIM,
                        cnn_filters=CNN_FILTERS,
                        dnn_units=DNN_UNITS,
                        model_output_classes=OUTPUT_CLASSES,
                        dropout_rate=DROPOUT_RATE)

# specify the loss function. we used binary_crossentropy
if OUTPUT_CLASSES == 2:
    text_model.compile(loss="binary_crossentropy",
                       optimizer="adam",
                       metrics=["accuracy"])
else:
    text_model.compile(loss="sparse_categorical_crossentropy",
                       optimizer="adam",
                       metrics=["sparse_categorical_accuracy"]) 

text_model.fit(train_data, epochs=NB_EPOCHS)  # fit the network


results_cnn = text_model.predict(test_data) # predict 
scores = text_model.evaluate(test_data)

print("Large CNN Accuracy: %.2f%%" % (scores[1]*100))

# here is a function to evaluate the model using ROC and AUC. Not necessary though since we arent using it.
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
from itertools import chain

def plot_metrics(pred, true_labels):
    """Plots a ROC curve with the accuracy and the AUC"""
    acc = accuracy_score(true_labels, np.array(pred >= .5, dtype='int'))
    fpr, tpr, thresholds = roc_curve(true_labels, pred)
    auc = roc_auc_score(true_labels, pred)

    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.plot(fpr, tpr, color='red')
    ax.plot([0,1], [0,1], color='black', linestyle='--')
    ax.set_title(f"AUC: {auc}\nACC: {acc}");
    return fig

test_labels = []
for images, label in test_data.take(2000): 
  test_labels.append(label.numpy())

test_labels_y = list(chain(*test_labels))
print(results_cnn)
plot_metrics(results_cnn, test_labels_y)

# function to plot a confusion matrix. This is from SKLEARN library
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import itertools

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('coolwarm')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    # plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     fontsize=12, fontweight="medium",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label') 
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

cm = confusion_matrix(test_labels_y, np.array(results_cnn >= .5, dtype='int'))
labels = ["Genuine",  "Fake"]
print(cm)
plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

"""LSTM"""

# here we train LSTM
import keras

# define the prameters for LSTM
VOCAB_LENGTH = len(tokenizer.vocab) # length of input embedding from BERT
EMB_DIM = 200 # dimension of embedding output
CNN_FILTERS = 100
DNN_UNITS = 256 # LSTM Units
OUTPUT_CLASSES = 2

DROPOUT_RATE = 0.2

NB_EPOCHS = 15 

# make the layers of LSTM
inputs = keras.Input(shape=(None,), dtype="int32")
x = layers.Embedding(input_dim=VOCAB_LENGTH, output_dim=EMB_DIM, mask_zero=True)(inputs)
x1 = layers.SpatialDropout1D(0.5)(x)
x2 = layers.LSTM(64, return_sequences=False)(x1)
x3 = layers.Dense(128, activation='relu')(x2)
outputs = layers.Dense(1,activation='sigmoid')(x3)

model = keras.Model(inputs, outputs)

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(train_data, epochs=NB_EPOCHS) # fit the network




results_lstm = model.predict(test_data) # predict from the trained model
# print(results)

# results = model.predict(test_data)
scores = model.evaluate(test_data)

print("Large LSTM Accuracy: %.2f%%" % (scores[1]*100))

from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
from itertools import chain

def plot_metrics(pred, true_labels):
    """Plots a ROC curve with the accuracy and the AUC"""
    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))
    fpr, tpr, thresholds = roc_curve(true_labels, pred)
    auc = roc_auc_score(true_labels, pred)

    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.plot(fpr, tpr, color='red')
    ax.plot([0,1], [0,1], color='black', linestyle='--')
    ax.set_title(f"AUC: {auc}\nACC: {acc}");
    return fig

test_labels = []
for images, label in test_data.take(2000): 
  test_labels.append(label.numpy())

test_labels_y = list(chain(*test_labels))

plot_metrics(results_lstm, test_labels_y)

# plot the confusion matrix
cm = confusion_matrix(test_labels_y, np.array(results_lstm >= .5, dtype='int'))
labels = ["Genuine",  "Fake"]
print(cm)
plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

# https://towardsdatascience.com/implementing-naive-bayes-algorithm-from-scratch-python-c6880cfc9c41
# Bayesian Model starts
# prior probabilities
class Bayes:

  def __init__(self):
    pass

  # calcualte the prior probabilties
  def calculate_prior_dist(self, features, target):
      self.prior = (features.groupby(target).apply(lambda x: len(x))/self.rows).to_numpy()
      return self.prior
  
  # get mean and variance
  def calc_statistics(self, features, target):
        self.mean = features.groupby(target).apply(np.mean).to_numpy()
        self.var = features.groupby(target).apply(np.var).to_numpy()
              
        return self.mean, self.var
  
  # estimate the posterior probabilites with gaussian density
  def calc_posterior(self, x):
      posteriors = []
      for i in range(self.count):
          prior = np.log(self.prior[i]) 
          conditional = np.sum(np.log(self.gaussian_density(i, x)))
          posterior = prior + conditional
          posteriors.append(posterior)
      return self.classes[np.argmax(posteriors)]

    
  def gaussian_density(self, class_idx, x):     

      mean = self.mean[class_idx]
      var = self.var[class_idx]
      numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))
      denominator = np.sqrt(2 * np.pi * var)
      prob = numerator / denominator
      return prob

  # fit bayesian model
  def fit(self, features, target):
      # define class variables 
      self.classes = np.unique(target)
      self.count = len(self.classes)
      self.feature_nums = features.shape[1]
      self.rows = features.shape[0]
      
      # calculate statistics    
      self.calc_statistics(features, target)
      self.calculate_prior_dist(features, target)
          
  # predict from the model
  def predict(self, features):
      preds = [self.calc_posterior(f) for f in features.to_numpy()]
      return preds

from sklearn.model_selection import train_test_split

# combine the scores from BERT-CNN and BERT-LSTM
xnew = pd.DataFrame(results_cnn)
xnew.columns = ['results_cnn']
xnew['results_lstm'] = results_lstm 
yy = pd.DataFrame(test_labels_y)
X_train, X_test, y_train, y_test = train_test_split(xnew, yy, test_size=0.33, random_state=42)
print(y_train)

# fit bayes model
bys = Bayes()
bys.fit(X_train, y_train[0]) # train the model with the scores
outputs = bys.predict(X_test) # predict
acc = accuracy_score(y_test, outputs)
print(acc) 

# plot confusion matrix
cm = confusion_matrix(y_test, outputs)
labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

# SUM Rule
zipped_lists = zip(results_cnn, results_lstm)
xnew = np.array([x + y for (x, y) in zipped_lists])

# xnew = np.sum(results_cnn,results_lstm)
print(yy)
yy = pd.DataFrame(test_labels_y)

acc = accuracy_score(yy, np.array(xnew > 0.5, dtype='int'))

cm = confusion_matrix(yy, np.array(xnew > 0.5, dtype='int'))

labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

# Weighted SUM Rule
n_results_cnn = .9*(results_cnn)
n_results_lstm = .1*(results_lstm)
n_results_cnn = (n_results_cnn - np.min(n_results_cnn)/ (np.max(n_results_cnn) - np.min(n_results_cnn)))
n_results_lstm = (n_results_lstm - np.min(n_results_lstm)/ (np.max(n_results_lstm) - np.min(n_results_lstm)))

zipped_lists = zip(n_results_cnn, n_results_lstm)
xnew = np.array([x + y for (x, y) in zipped_lists])

# xnew = np.sum(results_cnn,results_lstm)
# print(xnew)
yy = pd.DataFrame(test_labels_y)
# X_train, X_test, y_train, y_test = train_test_split(xnew, yy, test_size=0.33, random_state=42)
# print(y_train) 
acc = accuracy_score(yy, xnew >= .5)
# print(acc) 


cm = confusion_matrix(yy, xnew >= .5)

labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)