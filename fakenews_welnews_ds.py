# -*- coding: utf-8 -*-
"""FakeNews_WELNews_DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/141kxigfaHxCbhXSWrRiWfBJ0ljM_vHPY

**Author Shafqaat Ahmad@PennState** Fake News detection using WELNews dataset

# BERT-CNN
"""

!pip install bert-for-tf2
!pip install sentencepiece
import datetime

# Commented out IPython magic to ensure Python compatibility.
try:
#     %tensorflow_version 2.x
except Exception:
    pass
import tensorflow as tf

import tensorflow_hub as hub

from tensorflow.keras import layers
import bert
import pandas as pd
import numpy as np
import re
import random
import math
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

iterationcount=0


df = pd.DataFrame ( columns = ['Itearation No','BERT-CNN-Genuine','BERT-CNN-Fake','BERT-CNN-Total','BERT-LSTM-Genuine','BERT-LSTM-Fake','BERT-LSTM-Total','SUM-Genuine','SUM-Fake','SUM-Total','Bayes-Genuine','Bayes-Fake','Bayes-Total'])

import random as rn






# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(78802)



# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(678245)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,
                                        inter_op_parallelism_threads=1)

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see:
# https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.compat.v1.set_random_seed(678245)

sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

#from google.colab import drive
#drive.mount('/content/drive')

fake_news = pd.read_csv("/content/drive/MyDrive/FakeNewsDataSets/WELFake_Dataset.csv") 

fake_news.isnull().values.any()

print(fake_news.shape)

fake_news = fake_news.sample(frac=0.06) #0.06 Select 6% of dataset
fake_news.describe()
fake_news.isnull().sum()

fake_news.dropna(inplace=True)
fake_news.isnull().sum()

print(fake_news.head)

sns.countplot(x="label", data=fake_news);
plt.show()

#from google.colab import drive
#drive.mount('/content/drive')

def preprocess_text(sen):
    # Removing html tags
    sentence = remove_tags(sen)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', str(text)) 

news = []
sentences = list(fake_news['text'])
for sen in sentences:
    news.append(preprocess_text(sen)) 

print(fake_news.columns.values)


  fake_news.label.unique()

y = fake_news['label']
y = np.array(list(map(lambda x: 1 if x==1 else 0, y)))


print(news[10])  

BertTokenizer = bert.bert_tokenization.FullTokenizer
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2",
                            trainable=False)
vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = BertTokenizer(vocabulary_file, to_lower_case)

def tokenize_reviews(text_news):
    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_news)) 

tokenized_news = [tokenize_reviews(row) for row in news]
print(tokenized_news[1:2])
# print(news[1:2])

news_with_len = [[news_j, y[i], len(news_j)] for i, news_j in enumerate(tokenized_news)]

random.shuffle(news_with_len)

news_with_len.sort(key=lambda x: x[2])

sorted_news_labels = [(news_lab[0], news_lab[1]) for news_lab in news_with_len]

processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_news_labels, output_types=(tf.int32, tf.int32))
# print(processed_dataset.batch(64).as_numpy_iterator())

BATCH_SIZE = 128
batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))

next(iter(batched_dataset))

TOTAL_BATCHES = math.ceil(len(sorted_news_labels) / BATCH_SIZE)
print(TOTAL_BATCHES)
TEST_BATCHES = TOTAL_BATCHES // 4
print(TEST_BATCHES)
batched_dataset.shuffle(TOTAL_BATCHES)
test_data = batched_dataset.take(TEST_BATCHES)
train_data = batched_dataset.skip(TEST_BATCHES)

"""CNN"""

class TEXT_MODEL_CNN(tf.keras.Model):
    
    def __init__(self,
                vocabulary_size,
                embedding_dimensions=128,
                cnn_filters=100,
                dnn_units=512,
                model_output_classes=2,
                dropout_rate=0.5,
                training=False,
                name="text_model"):
        super(TEXT_MODEL_CNN, self).__init__(name=name)
        
        self.embedding = layers.Embedding(vocabulary_size,
                                          embedding_dimensions)
        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=2,
                                        padding="valid",
                                        activation="relu")
        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=3,
                                        padding="valid",
                                        activation="relu")
        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,
                                        kernel_size=4,
                                        padding="valid",
                                        activation="relu")
        self.pool = layers.GlobalMaxPool1D()
        
        self.dense_1 = layers.Dense(units=dnn_units, activation="relu")
        self.dropout = layers.Dropout(rate=dropout_rate)
        if model_output_classes == 2:
            self.last_dense = layers.Dense(units=1,
                                          activation="sigmoid")
        else:
            self.last_dense = layers.Dense(units=model_output_classes,
                                          activation="softmax")
    
    def call(self, inputs, training):
        l = self.embedding(inputs)
        l_1 = self.cnn_layer1(l) 
        l_1 = self.pool(l_1) 
        l_2 = self.cnn_layer2(l) 
        l_2 = self.pool(l_2)
        l_3 = self.cnn_layer3(l)
        l_3 = self.pool(l_3) 
        
        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)
        concatenated = self.dense_1(concatenated)
        concatenated = self.dropout(concatenated, training)
        model_output = self.last_dense(concatenated)
        
        return model_output

VOCAB_LENGTH = len(tokenizer.vocab)
EMB_DIM = 500
CNN_FILTERS = 100
DNN_UNITS = 256
OUTPUT_CLASSES = 2

DROPOUT_RATE = 0.2

NB_EPOCHS = 10


text_model = TEXT_MODEL_CNN(vocabulary_size=VOCAB_LENGTH,
                        embedding_dimensions=EMB_DIM,
                        cnn_filters=CNN_FILTERS,
                        dnn_units=DNN_UNITS,
                        model_output_classes=OUTPUT_CLASSES,
                        dropout_rate=DROPOUT_RATE)

if OUTPUT_CLASSES == 2:
    text_model.compile(loss="binary_crossentropy",
                      optimizer="adam",
                      metrics=["accuracy"])
else:
    text_model.compile(loss="sparse_categorical_crossentropy",
                      optimizer="adam",
                      metrics=["sparse_categorical_accuracy"]) 

text_model.fit(train_data, epochs=NB_EPOCHS) 

results_cnn = text_model.predict(test_data)
scores = text_model.evaluate(test_data)

print("Large CNN Accuracy: %.2f%%" % (scores[1]*100))

from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
from itertools import chain


def plot_metrics(pred, true_labels):
    """Plots a ROC curve with the accuracy and the AUC"""
    
    acc = accuracy_score(true_labels, np.array(pred >= .5, dtype='int'))
    
    df.loc[iterationcount,'Itearation No']=iterationcount
    df.loc[iterationcount,'BERT-CNN-Total']=round(acc,2)
    
    fpr, tpr, thresholds = roc_curve(true_labels, pred)
    auc = roc_auc_score(true_labels, pred)


    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.plot(fpr, tpr, color='red')
    ax.plot([0,1], [0,1], color='black', linestyle='--')
    ax.set_title(f"AUC: {auc}\nACC: {acc}");
    return fig

test_labels = []
for images, label in test_data.take(2000): 
  test_labels.append(label.numpy())

test_labels_y = list(chain(*test_labels))

plot_metrics(results_cnn, test_labels_y)

from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import itertools

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
          
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Greys')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    # plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                    horizontalalignment="center",
                    fontsize=12, fontweight="medium",
                    color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                    horizontalalignment="center",
                    color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label') 
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

cm = confusion_matrix(test_labels_y, np.array(results_cnn >= .5, dtype='int'))
print(cm)

cmdf=pd.DataFrame(cm)
cmdf=cmdf/cmdf.sum(axis=1)

print (cmdf)

#print (round(acccnn,2)) # Accuracy
print (round(cmdf.iloc[0,0],2)) # TP
print (round(cmdf.iloc[1,1],2)) # TN

#df.loc[iterationcount,'Itearation No']=iterationcount
df.loc[iterationcount,'BERT-CNN-Genuine']=round(cmdf.iloc[0,0],2)
df.loc[iterationcount,'BERT-CNN-Fake']=round(cmdf.iloc[1,1],2)
#df.loc[iterationcount,'BERT-CNN-Total']=round(acccnn,2)
print (df)


labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

"""LSTM"""

import keras
VOCAB_LENGTH = len(tokenizer.vocab)
EMB_DIM = 200
CNN_FILTERS = 100
DNN_UNITS = 256
OUTPUT_CLASSES = 2

DROPOUT_RATE = 0.2

NB_EPOCHS = 15 


inputs = keras.Input(shape=(None,), dtype="int32")
x = layers.Embedding(input_dim=VOCAB_LENGTH, output_dim=EMB_DIM, mask_zero=True)(inputs)
x1 = layers.SpatialDropout1D(0.5)(x)
x2 = layers.LSTM(64, return_sequences=False)(x1)
x3 = layers.Dense(128, activation='relu')(x2)
outputs = layers.Dense(1,activation='sigmoid')(x3)

model = keras.Model(inputs, outputs)

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(train_data, epochs=NB_EPOCHS)


results_lstm = model.predict(test_data)
# print(results)

# results = model.predict(test_data)
scores = model.evaluate(test_data)

print("Large LSTM Accuracy: %.2f%%" % (scores[1]*100))

from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
from itertools import chain



def plot_metrics(pred, true_labels):
    """Plots a ROC curve with the accuracy and the AUC"""
    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))
    
    df.loc[iterationcount,'BERT-LSTM-Total']=round(acc,2)
  
    
    fpr, tpr, thresholds = roc_curve(true_labels, pred)
    auc = roc_auc_score(true_labels, pred)

    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.plot(fpr, tpr, color='red')
    ax.plot([0,1], [0,1], color='black', linestyle='--')
    ax.set_title(f"AUC: {auc}\nACC: {acc}");
    return fig

test_labels = []
for images, label in test_data.take(2000): 
  test_labels.append(label.numpy())

test_labels_y = list(chain(*test_labels))

plot_metrics(results_lstm, test_labels_y)

cm = confusion_matrix(test_labels_y, np.array(results_lstm >= .5, dtype='int'))

print(cm)

cmdf=pd.DataFrame(cm)
cmdf=cmdf/cmdf.sum(axis=1)

print (cmdf)

#print (round(acclstm,2)) # Accuracy
print (round(cmdf.iloc[0,0],2)) # TP
print (round(cmdf.iloc[1,1],2)) # TN

df.loc[iterationcount,'BERT-LSTM-Genuine']=round(cmdf.iloc[0,0],2)
df.loc[iterationcount,'BERT-LSTM-Fake']=round(cmdf.iloc[1,1],2)
#df.loc[iterationcount,'BERT-LSTM-Total']=round(acc,2)
print (df)
labels = ["Genuine",  "Fake"]
print(cm)
plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

# https://towardsdatascience.com/implementing-naive-bayes-algorithm-from-scratch-python-c6880cfc9c41
# Bayes
class Bayes:

  def __init__(self):
    pass

  def calculate_prior_dist(self, features, target):
      self.prior = (features.groupby(target).apply(lambda x: len(x))/self.rows).to_numpy()
      return self.prior
  
  def calc_statistics(self, features, target):
        self.mean = features.groupby(target).apply(np.mean).to_numpy()
        self.var = features.groupby(target).apply(np.var).to_numpy()
              
        return self.mean, self.var
      
  def calc_posterior(self, x):
      posteriors = []
      for i in range(self.count):
          prior = np.log(self.prior[i]) 
          conditional = np.sum(np.log(self.gaussian_density(i, x)))
          posterior = prior + conditional
          posteriors.append(posterior)
      return self.classes[np.argmax(posteriors)]

    
  def gaussian_density(self, class_idx, x):     

      mean = self.mean[class_idx]
      var = self.var[class_idx]
      numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))
      denominator = np.sqrt(2 * np.pi * var)
      prob = numerator / denominator
      return prob

  def fit(self, features, target):
      # define class variables 
      self.classes = np.unique(target)
      self.count = len(self.classes)
      self.feature_nums = features.shape[1]
      self.rows = features.shape[0]
      
      # calculate statistics    
      self.calc_statistics(features, target)
      self.calculate_prior_dist(features, target)
          
  def predict(self, features):
      preds = [self.calc_posterior(f) for f in features.to_numpy()]
      return preds

from sklearn.model_selection import train_test_split

xnew = pd.DataFrame(results_cnn)
xnew.columns = ['results_cnn']
xnew['results_lstm'] = results_lstm 
yy = pd.DataFrame(test_labels_y)
X_train, X_test, y_train, y_test = train_test_split(xnew, yy, test_size=0.33, random_state=42)
print(y_train)
bys = Bayes()
bys.fit(X_train, y_train[0])
outputs = bys.predict(X_test)
acc = accuracy_score(y_test, outputs)

cm = confusion_matrix(y_test, outputs)
labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

cmdf=pd.DataFrame(cm)
cmdf=cmdf/cmdf.sum(axis=1)

print (cmdf)

print (round(acc,2)) # Accuracy
print (round(cmdf.iloc[0,0],2)) # TP
print (round(cmdf.iloc[1,1],2)) # TN

df.loc[iterationcount,'Bayes-Genuine']=round(cmdf.iloc[0,0],2)
df.loc[iterationcount,'Bayes-Fake']=round(cmdf.iloc[1,1],2)
df.loc[iterationcount,'Bayes-Total']=round(acc,2)
print (df)

# SUM Rule
zipped_lists = zip(results_cnn, results_lstm)
xnew = np.array([x + y for (x, y) in zipped_lists])

# xnew = np.sum(results_cnn,results_lstm)
print(yy)
yy = pd.DataFrame(test_labels_y)
# X_train, X_test, y_train, y_test = train_test_split(xnew, yy, test_size=0.33, random_state=42)
# print(y_train) 
acc = accuracy_score(yy, np.array(xnew > 0.5, dtype='int'))

print(acc) 


cm = confusion_matrix(yy, np.array(xnew > 0.5, dtype='int'))

labels = ["Genuine",  "Fake"]

plot_confusion_matrix(cm,labels,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True)

cmdf=pd.DataFrame(cm)
cmdf=cmdf/cmdf.sum(axis=1)
  
print (cmdf)

print (round(acc,2)) # Accuracy
print (round(cmdf.iloc[0,0],2)) # TP
print (round(cmdf.iloc[1,1],2)) # TN

df.loc[iterationcount,'SUM-Genuine']=round(cmdf.iloc[0,0],2)
df.loc[iterationcount,'SUM-Fake']=round(cmdf.iloc[1,1],2)
df.loc[iterationcount,'SUM-Total']=round(acc,2)
print (df)

# Weighted SUM Rule
  #n_results_cnn = .5*(results_cnn)
  #n_results_lstm = .5*(results_lstm)
  #n_results_cnn = (n_results_cnn - np.min(n_results_cnn)/ (np.max(n_results_cnn) - np.min(n_results_cnn)))
  #n_results_lstm = (n_results_lstm - np.min(n_results_lstm)/ (np.max(n_results_lstm) - np.min(n_results_lstm)))

  #zipped_lists = zip(n_results_cnn, n_results_lstm)
  #xnew = np.array([x + y for (x, y) in zipped_lists])

  ## xnew = np.sum(results_cnn,results_lstm)
  ## print(xnew)
  #yy = pd.DataFrame(test_labels_y)
  ## X_train, X_test, y_train, y_test = train_test_split(xnew, yy, test_size=0.33, random_state=42)
  ## print(y_train) 
  #acc = accuracy_score(yy, xnew >= .5)
  ## print(acc) 


  #cm = confusion_matrix(yy, xnew >= .5)

  #labels = ["Genuine",  "Fake"]

  #plot_confusion_matrix(cm,labels,
  #                         title='Confusion matrix',
  #                          cmap=None,
  #                          normalize=True)
  #

now = datetime.datetime.now()
print ("Current date and time : ")
print (now.strftime("%Y-%m-%d %H:%M:%S"))
print (df)

from google.colab import files
print (str(now))
df.to_csv('WELFAKE'+str(now)+'.csv') 
files.download('WELFAKE'+str(now)+'.csv')